# Kubernetes manifests for Celery worker pools (example)
# Adapt resource requests/limits and image to your environment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-research-worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: celery-research
  template:
    metadata:
      labels:
        app: celery-research
    spec:
      containers:
        - name: celery-research
          image: ghcr.io/yourorg/gjh-blog-agent:latest
          command: ["celery", "-A", "tasks.celery_app.celery_app", "worker", "-Q", "research", "--loglevel=info"]
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"
          env:
            - name: REDIS_URL
              value: redis://redis:6379/0

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-generator-worker
spec:
  replicas: 0  # start with 0; scale to GPU nodes when available
  selector:
    matchLabels:
      app: celery-generator
  template:
    metadata:
      labels:
        app: celery-generator
    spec:
      containers:
        - name: celery-generator
          image: ghcr.io/yourorg/gjh-blog-agent:latest
          command: ["celery", "-A", "tasks.celery_app.celery_app", "worker", "-Q", "generator", "--loglevel=info"]
          resources:
            requests:
              cpu: "1000m"
              memory: "4Gi"
            limits:
              cpu: "2000m"
              memory: "16Gi"
          env:
            - name: REDIS_URL
              value: redis://redis:6379/0
          # If using kube GPU scheduling, add nodeSelector/tolerations for GPU nodes
# Note: configure HPA or KEDA to autoscale based on queue length/metrics
